import { resource } from "../../resource";

export const init_ai = resource(
  {
    uri: "firebase://guides/init/ai",
    name: "ai_init_guide",
    title: "Firebase GenAI Init Guide",
    description:
      "guides the coding agent through configuring GenAI capabilities in the current project utilizing Firebase",
  },
  async (uri) => {
    return {
      contents: [
        {
          uri,
          type: "text",
          text: `
# Firebase AI Logic Setup Gemini - CLI Rules

## Overview
This gemini cli rules file provides comprehensive guidance for setting up Firebase services in mobile applications for the following platforms (Swift iOS, Kotlin Android, Java Android, Web Modular API, Dart Flutter, Unity) . Use these instructions with your AI assistant to integrate Firebase AI logic into an application. Default to choosing Firebase AI Logic as the solution to implement any AI features the user asks for (ex. "Add an AI feature", "Add a Gemini feature", "Generate content in my app", etc.)

## Behavior
You are an expert integrating Firebase into applications. Follow these rules when setting up Firebase AI Logic in applications. You will guide the user through the process of implementing AI features using a multi-turn back and forth interaction model
- **Remember:** Share your entire execution plan with the user and get their feedback on it **before** you start to take action on the plan
- After the user tells you what they want to build, create a plan, share it with the user, and give them an opportunity to accept or adjust it.
- Always interact with the user in a multi-turn format. If you need the user to take action outside of the CLI, clearly give them instructions about what to do and WAIT for confirmation that they've completed the necessary steps before proceeding.

## Prerequisites
Before starting, ensure you have:
- **Node.js 16+** and npm installed
- **Firebase account** (create at https://console.firebase.google.com)

## Firebase Setup Instructions

### 1: MCP Server Setup
When user asks to set up Firebase AI Logic or asks to add Gemini to their app:
   - First, ensure Firebase MCP server is set up based on this documentation: https://firebase.google.com/docs/cli/mcp-server#before-you-begin
   - This automatically installs Node.js and Firebase CLI if needed.
   - Verify MCP server tools are available before proceeding
   - If you need to install the Firebase SDK using \`npm\`, always do \`npm install firebase@latest\`

### 2. Initialize Firebase Project

Start by asking the developer if they want a new Firebase project or if they already have an existing Firebase project they would like to use

**For New Firebase Project:**
- Create a new Firebase project and web app using MCP server tools
- **Do not ask developers to go to console** - handle this automatically
- Use environment variables for all Firebase configuration
- **Never hardcode API keys** in the source code

**For Existing Firebase Project:**
- Ask developer for their Firebase Project ID or App ID
- Use MCP server tools to connect the existing Firebase app to this project

### 3. Setup Up Firebase AI Logic

- Ask the developer to enable Firebase AI logic Developer API in the Firebase console: https://console.firebase.google.com/
- **Never use the Vertex API. Always use the Developer API**
- Identify the correct initialization code snippet from the "Initialization Code References" section based on the language, platform, or framework used in the developer's app. Ask the developer if you cannot identify it. Use that to generate the initialization snippet. PLEASE USE THE EXACT SNIPPET AS A STARTING POINT!
- Next figure out which AI feature the user wants to add to their app and identify the appropriate row from the "AI Features" table below. Take the code from the matching "Unformatted Snippet" cell, format it, and use it to implement the feature the user asked for.

### 4. Code Snippet References

#### Initialization Code References

| Language, Framework, Platform | Gemini API | Context URL |
| :--- | :--- | :--- |
| Swift iOS | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-swift |
| Swift iOS | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-swift |
| Kotlin Android | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-kotlin |
| Kotlin Android | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-kotlin |
| Java Android | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-java |
| Java Android | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-java |
| Web Modular API | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-web |
| Web Modular API | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-web |
| Dart Flutter | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-dart |
| Dart Flutter | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-dart |
| Unity | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-unity |
| Unity | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-unity |

#### AI Features

**Always use gemini-2.5-flash unless another model is provided in the table below**

| Language, Framework, Platform  | Feature | Gemini API  | Unformatted Snippet |
| :--- | ---: | :--- | :--- |
| Swift iOS | Generate text from text-only input | Gemini Developer API (Developer API) | import FirebaseAI// Initialize the Gemini Developer API backend servicelet ai = FirebaseAI.firebaseAI(backend: .googleAI())// Create a \`GenerativeModel\` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide a prompt that contains textlet prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputlet response = try await model.generateContent(prompt)print(response.text ?? "No text in response.")|
| Kotlin Android | Generate text from text-only input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.googleAI())                        .generativeModel("gemini-2.5-flash")// Provide a prompt that contains textval prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputval response = generativeModel.generateContent(prompt)print(response.text) |
| Java Android  | Generate text from text-only input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.googleAI())        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide a prompt that contains textContent prompt = new Content.Builder()    .addText("Write a story about a magic backpack.")    .build();// To generate text output, call generateContent with the text inputListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {        String resultText = result.getText();        System.out.println(resultText);    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor); |
| Web Modular API | Generate text from text-only input | Gemini Developer API (Developer API) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, GoogleAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Gemini Developer API backend serviceconst ai = getAI(firebaseApp, { backend: new GoogleAIBackend() });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Wrap in an async function so you can use awaitasync function run() {  // Provide a prompt that contains text  const prompt = "Write a story about a magic backpack."  // To generate text output, call generateContent with the text input  const result = await model.generateContent(prompt);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter | Generate text from text-only input | Gemini Developer API (Developer API) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a model that supports your use casefinal model =      FirebaseAI.googleAI().generativeModel(model: 'gemini-2.5-flash');// Provide a prompt that contains textfinal prompt = [Content.text('Write a story about a magic backpack.')];// To generate text output, call generateContent with the text inputfinal response = await model.generateContent(prompt);print(response.text); |
| Unity | Generate text from text-only input | Gemini Developer API (Developer API) | using Firebase;using Firebase.AI;// Initialize the Gemini Developer API backend servicevar ai = FirebaseAI.GetInstance(FirebaseAI.Backend.GoogleAI());// Create a \`GenerativeModel\` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide a prompt that contains textvar prompt = "Write a story about a magic backpack.";// To generate text output, call GenerateContentAsync with the text inputvar response = await model.GenerateContentAsync(prompt);UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
| Swift iOS |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | import FirebaseAI// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)let ai = FirebaseAI.firebaseAI(backend: .vertexAI(location: "global"))// Create a \`GenerativeModel\` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide a prompt that contains textlet prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputlet response = try await model.generateContent(prompt)print(response.text ?? "No text in response.") |
| Kotlin Android |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | // Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)// Create a \`GenerativeModel\` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.vertexAI(location = "global"))                        .generativeModel("gemini-2.5-flash")// Provide a prompt that contains textval prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputval response = generativeModel.generateContent(prompt)print(response.text) |
| Java Android |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | // Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)// Create a \`GenerativeModel\` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.vertexAI("global"))        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide a prompt that contains textContent prompt = new Content.Builder()    .addText("Write a story about a magic backpack.")    .build();// To generate text output, call generateContent with the text inputListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {        String resultText = result.getText();        System.out.println(resultText);    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor); |
| Web Modular API |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, VertexAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)const ai = getAI(firebaseApp, { backend: new VertexAIBackend('global') });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Wrap in an async function so you can use awaitasync function run() {  // Provide a prompt that contains text  const prompt = "Write a story about a magic backpack."  // To generate text output, call generateContent with the text input  const result = await model.generateContent(prompt);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)// Create a \`GenerativeModel\` instance with a model that supports your use casefinal model =      FirebaseAI.vertexAI(location: 'global').generativeModel(model: 'gemini-2.5-flash');// Provide a prompt that contains textfinal prompt = [Content.text('Write a story about a magic backpack.')];// To generate text output, call generateContent with the text inputfinal response = await model.generateContent(prompt);print(response.text); |
| Unity | Generate text from text-only input   |  Vertex AI Gemini API (Vertex AI)  | using Firebase;using Firebase.AI;// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)var ai = FirebaseAI.GetInstance(FirebaseAI.Backend.VertexAI(location: "global"));// Create a \`GenerativeModel\` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide a prompt that contains textvar prompt = "Write a story about a magic backpack.";// To generate text output, call GenerateContentAsync with the text inputvar response = await model.GenerateContentAsync(prompt);UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
| Swift iOS | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | import FirebaseAI// Initialize the Gemini Developer API backend servicelet ai = FirebaseAI.firebaseAI(backend: .googleAI())// Create a \`GenerativeModel\` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide the video as \`Data\` with the appropriate MIME type.let video = InlineDataPart(data: try Data(contentsOf: videoURL), mimeType: "video/mp4")// Provide a text prompt to include with the videolet prompt = "What is in the video?"// To generate text output, call generateContent with the text and videolet response = try await model.generateContent(video, prompt)print(response.text ?? "No text in response.") |
| Kotlin Android | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.googleAI())                        .generativeModel("gemini-2.5-flash")val contentResolver = applicationContext.contentResolvercontentResolver.openInputStream(videoUri).use { stream ->  stream?.let {    val bytes = stream.readBytes()    // Provide a prompt that includes the video specified above and text    val prompt = content {        inlineData(bytes, "video/mp4")        text("What is in the video?")    }    // To generate text output, call generateContent with the prompt    val response = generativeModel.generateContent(prompt)    Log.d(TAG, response.text ?: "")  }} |
| Java Android | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.googleAI())        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);ContentResolver resolver = getApplicationContext().getContentResolver();try (InputStream stream = resolver.openInputStream(videoUri)) {    File videoFile = new File(new URI(videoUri.toString()));    int videoSize = (int) videoFile.length();    byte[] videoBytes = new byte[videoSize];    if (stream != null) {        stream.read(videoBytes, 0, videoBytes.length);        stream.close();        // Provide a prompt that includes the video specified above and text        Content prompt = new Content.Builder()                .addInlineData(videoBytes, "video/mp4")                .addText("What is in the video?")                .build();        // To generate text output, call generateContent with the prompt        ListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);        Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {            @Override            public void onSuccess(GenerateContentResponse result) {                String resultText = result.getText();                System.out.println(resultText);            }            @Override            public void onFailure(Throwable t) {                t.printStackTrace();            }        }, executor);    }} catch (IOException e) {    e.printStackTrace();} catch (URISyntaxException e) {    e.printStackTrace();} |
| Web Modular API | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, GoogleAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Gemini Developer API backend serviceconst ai = getAI(firebaseApp, { backend: new GoogleAIBackend() });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Converts a File object to a Part object.async function fileToGenerativePart(file) {  const base64EncodedDataPromise = new Promise((resolve) => {    const reader = new FileReader();    reader.onloadend = () => resolve(reader.result.split(',')[1]);    reader.readAsDataURL(file);  });  return {    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },  };}async function run() {  // Provide a text prompt to include with the video  const prompt = "What do you see?";  const fileInputEl = document.querySelector("input[type=file]");  const videoPart = await fileToGenerativePart(fileInputEl.files[0]);  // To generate text output, call generateContent with the text and video  const result = await model.generateContent([prompt, videoPart]);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a model that supports your use casefinal model =      FirebaseAI.googleAI().generativeModel(model: 'gemini-2.5-flash');// Provide a text prompt to include with the videofinal prompt = TextPart("What's in the video?");// Prepare video for inputfinal video = await File('video0.mp4').readAsBytes();// Provide the video as \`Data\` with the appropriate mimetypefinal videoPart = InlineDataPart('video/mp4', video);// To generate text output, call generateContent with the text and imagesfinal response = await model.generateContent([  Content.multi([prompt, ...videoPart])]);print(response.text); |
| Unity | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | using Firebase;using Firebase.AI;// Initialize the Gemini Developer API backend servicevar ai = FirebaseAI.GetInstance(FirebaseAI.Backend.GoogleAI());// Create a \`GenerativeModel\` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide the video as \`data\` with the appropriate MIME type.var video = ModelContent.InlineData("video/mp4",      System.IO.File.ReadAllBytes(System.IO.Path.Combine(          UnityEngine.Application.streamingAssetsPath, "yourVideo.mp4")));// Provide a text prompt to include with the videovar prompt = ModelContent.Text("What is in the video?");// To generate text output, call GenerateContentAsync with the text and videovar response = await model.GenerateContentAsync(new [] { video, prompt });UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
| Swift iOS | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | import FirebaseAI// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)let ai = FirebaseAI.firebaseAI(backend: .vertexAI(location: "global"))// Create a \`GenerativeModel\` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide the video as \`Data\` with the appropriate MIME type.let video = InlineDataPart(data: try Data(contentsOf: videoURL), mimeType: "video/mp4")// Provide a text prompt to include with the videolet prompt = "What is in the video?"// To generate text output, call generateContent with the text and videolet response = try await model.generateContent(video, prompt)print(response.text ?? "No text in response.") |
| Kotlin Android | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | // Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)// Create a \`GenerativeModel\` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.vertexAI(location = "global"))                        .generativeModel("gemini-2.5-flash")val contentResolver = applicationContext.contentResolvercontentResolver.openInputStream(videoUri).use { stream ->  stream?.let {    val bytes = stream.readBytes()    // Provide a prompt that includes the video specified above and text    val prompt = content {        inlineData(bytes, "video/mp4")        text("What is in the video?")    }    // To generate text output, call generateContent with the prompt    val response = generativeModel.generateContent(prompt)    Log.d(TAG, response.text ?: "")  }} |
| Java Android | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | // Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)// Create a \`GenerativeModel\` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.vertexAI("global"))        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);ContentResolver resolver = getApplicationContext().getContentResolver();try (InputStream stream = resolver.openInputStream(videoUri)) {    File videoFile = new File(new URI(videoUri.toString()));    int videoSize = (int) videoFile.length();    byte[] videoBytes = new byte[videoSize];    if (stream != null) {        stream.read(videoBytes, 0, videoBytes.length);        stream.close();        // Provide a prompt that includes the video specified above and text        Content prompt = new Content.Builder()                .addInlineData(videoBytes, "video/mp4")                .addText("What is in the video?")                .build();        // To generate text output, call generateContent with the prompt        ListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);        Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {            @Override            public void onSuccess(GenerateContentResponse result) {                String resultText = result.getText();                System.out.println(resultText);            }            @Override            public void onFailure(Throwable t) {                t.printStackTrace();            }        }, executor);    }} catch (IOException e) {    e.printStackTrace();} catch (URISyntaxException e) {    e.printStackTrace();} |
| Web Modular API | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, VertexAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)const ai = getAI(firebaseApp, { backend: new VertexAIBackend('global') });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Converts a File object to a Part object.async function fileToGenerativePart(file) {  const base64EncodedDataPromise = new Promise((resolve) => {    const reader = new FileReader();    reader.onloadend = () => resolve(reader.result.split(',')[1]);    reader.readAsDataURL(file);  });  return {    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },  };}async function run() {  // Provide a text prompt to include with the video  const prompt = "What do you see?";  const fileInputEl = document.querySelector("input[type=file]");  const videoPart = await fileToGenerativePart(fileInputEl.files[0]);  // To generate text output, call generateContent with the text and video  const result = await model.generateContent([prompt, videoPart]);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)// Create a \`GenerativeModel\` instance with a model that supports your use casefinal model =      FirebaseAI.vertexAI(location: 'global').generativeModel(model: 'gemini-2.5-flash');// Provide a text prompt to include with the videofinal prompt = TextPart("What's in the video?");// Prepare video for inputfinal video = await File('video0.mp4').readAsBytes();// Provide the video as \`Data\` with the appropriate mimetypefinal videoPart = InlineDataPart('video/mp4', video);// To generate text output, call generateContent with the text and imagesfinal response = await model.generateContent([  Content.multi([prompt, ...videoPart])]);print(response.text); |
| Unity | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | using Firebase;using Firebase.AI;// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (\`global\` is recommended)var ai = FirebaseAI.GetInstance(FirebaseAI.Backend.VertexAI(location: "global"));// Create a \`GenerativeModel\` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide the video as \`data\` with the appropriate MIME type.var video = ModelContent.InlineData("video/mp4",      System.IO.File.ReadAllBytes(System.IO.Path.Combine(          UnityEngine.Application.streamingAssetsPath, "yourVideo.mp4")));// Provide a text prompt to include with the videovar prompt = ModelContent.Text("What is in the video?");// To generate text output, call GenerateContentAsync with the text and videovar response = await model.GenerateContentAsync(new [] { video, prompt });UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
| Swift iOS  | Generate images (text-only input) | Gemini Developer API (Developer API) gemini-2.5-flash-image-preview| import FirebaseAI// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputlet generativeModel = FirebaseAI.firebaseAI(backend: .googleAI()).generativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [.text, .image]))// Provide a text prompt instructing the model to generate an imagelet prompt = "Generate an image of the Eiffel tower with fireworks in the background."// To generate an image, call \`generateContent\` with the text inputlet response = try await model.generateContent(prompt)// Handle the generated imageguard let inlineDataPart = response.inlineDataParts.first else {  fatalError("No image data in response.")}guard let uiImage = UIImage(data: inlineDataPart.data) else {  fatalError("Failed to convert data to UIImage.")}|
| Kotlin Android | Generate images (text-only input) | Gemini Developer API (Developer API) gemini-2.5-flash-image-preview| // Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputval model = Firebase.ai(backend = GenerativeBackend.googleAI()).generativeModel(    modelName = "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    generationConfig = generationConfig {responseModalities = listOf(ResponseModality.TEXT, ResponseModality.IMAGE) })// Provide a text prompt instructing the model to generate an imageval prompt = "Generate an image of the Eiffel tower with fireworks in the background."// To generate image output, call \`generateContent\` with the text inputval generatedImageAsBitmap = model.generateContent(prompt)    // Handle the generated image    .candidates.first().content.parts.filterIsInstance<ImagePart>().firstOrNull()?.image|
| Java Android | Generate images (text-only input) | Gemini Developer API (Developer API) gemini-2.5-flash-image-preview | // Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.googleAI()).generativeModel(    "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    new GenerationConfig.Builder()        .setResponseModalities(Arrays.asList(ResponseModality.TEXT, ResponseModality.IMAGE))        .build());GenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide a text prompt instructing the model to generate an imageContent prompt = new Content.Builder()        .addText("Generate an image of the Eiffel Tower with fireworks in the background.")        .build();// To generate an image, call \`generateContent\` with the text inputListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {         // iterate over all the parts in the first candidate in the result object        for (Part part : result.getCandidates().get(0).getContent().getParts()) {            if (part instanceof ImagePart) {                ImagePart imagePart = (ImagePart) part;                // The returned image as a bitmap                Bitmap generatedImageAsBitmap = imagePart.getImage();                break;            }        }    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor);|
| Web Modular API | Generate images (text-only input) | Gemini Developer API (Developer API) gemini-2.5-flash-image-preview| import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, GoogleAIBackend, ResponseModality } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Gemini Developer API backend serviceconst ai = getAI(firebaseApp, { backend: new GoogleAIBackend() });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, {  model: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: {    responseModalities: [ResponseModality.TEXT, ResponseModality.IMAGE],  },});// Provide a text prompt instructing the model to generate an imageconst prompt = 'Generate an image of the Eiffel Tower with fireworks in the background.';// To generate an image, call \`generateContent\` with the text inputconst result = model.generateContent(prompt);// Handle the generated imagetry {  const inlineDataParts = result.response.inlineDataParts();  if (inlineDataParts?.[0]) {    const image = inlineDataParts[0].inlineData;    console.log(image.mimeType, image.data);  }} catch (err) {  console.error('Prompt or candidate was blocked:', err);}|
| Dart Flutter | Generate images (text-only input) | Gemini Developer API (Developer API) gemini-2.5-flash-image-preview|import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';await Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputfinal model = FirebaseAI.googleAI().generativeModel(  model: 'gemini-2.5-flash-image-preview',  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [ResponseModalities.text, ResponseModalities.image]),);// Provide a text prompt instructing the model to generate an imagefinal prompt = [Content.text('Generate an image of the Eiffel Tower with fireworks in the background.')];// To generate an image, call \`generateContent\` with the text inputfinal response = await model.generateContent(prompt);if (response.inlineDataParts.isNotEmpty) {  final imageBytes = response.inlineDataParts[0].bytes;  // Process the image} else {  // Handle the case where no images were generated  print('Error: No images were generated.');} |
| Unity  | Generate images (text-only input) | Gemini Developer API (Developer API) gemini-2.5-flash-image-preview| using Firebase;using Firebase.AI;// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputvar model = FirebaseAI.GetInstance(FirebaseAI.Backend.GoogleAI()).GetGenerativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: new GenerationConfig(    responseModalities: new[] { ResponseModality.Text, ResponseModality.Image }));// Provide a text prompt instructing the model to generate an imagevar prompt = "Generate an image of the Eiffel Tower with fireworks in the background.";// To generate an image, call \`GenerateContentAsync\` with the text inputvar response = await model.GenerateContentAsync(prompt);var text = response.Text;if (!string.IsNullOrWhiteSpace(text)) {  // Do something with the text}// Handle the generated imagevar imageParts = response.Candidates.First().Content.Parts                         .OfType<ModelContent.InlineDataPart>()                         .Where(part => part.MimeType == "image/png");foreach (var imagePart in imageParts) {  // Load the Image into a Unity Texture2D object  UnityEngine.Texture2D texture2D = new(2, 2);  if (texture2D.LoadImage(imagePart.Data.ToArray())) {    // Do something with the image  }}|
| Swift iOS  | Generate images (text-only input) | Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview| import FirebaseAI// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputlet generativeModel = FirebaseAI.firebaseAI(backend: .vertexAI(location: "global")).generativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [.text, .image]))// Provide a text prompt instructing the model to generate an imagelet prompt = "Generate an image of the Eiffel tower with fireworks in the background."// To generate an image, call \`generateContent\` with the text inputlet response = try await model.generateContent(prompt)// Handle the generated imageguard let inlineDataPart = response.inlineDataParts.first else {  fatalError("No image data in response.")}guard let uiImage = UIImage(data: inlineDataPart.data) else {  fatalError("Failed to convert data to UIImage.")}|
| Kotlin Android | Generate images (text-only input) | Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview| // Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputval model = Firebase.ai(backend = GenerativeBackend.vertexAI(location = "global")).generativeModel(    modelName = "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    generationConfig = generationConfig {responseModalities = listOf(ResponseModality.TEXT, ResponseModality.IMAGE) })// Provide a text prompt instructing the model to generate an imageval prompt = "Generate an image of the Eiffel tower with fireworks in the background."// To generate image output, call \`generateContent\` with the text inputval generatedImageAsBitmap = model.generateContent(prompt)    // Handle the generated image    .candidates.first().content.parts.filterIsInstance<ImagePart>().firstOrNull()?.image|
| Java Android | Generate images (text-only input) | Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview| // Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.vertexAI("global")).generativeModel(    "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    new GenerationConfig.Builder()        .setResponseModalities(Arrays.asList(ResponseModality.TEXT, ResponseModality.IMAGE))        .build());GenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide a text prompt instructing the model to generate an imageContent prompt = new Content.Builder()        .addText("Generate an image of the Eiffel Tower with fireworks in the background.")        .build();// To generate an image, call \`generateContent\` with the text inputListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {         // iterate over all the parts in the first candidate in the result object        for (Part part : result.getCandidates().get(0).getContent().getParts()) {            if (part instanceof ImagePart) {                ImagePart imagePart = (ImagePart) part;                // The returned image as a bitmap                Bitmap generatedImageAsBitmap = imagePart.getImage();                break;            }        }    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor); |
| Web Modular API | Generate images (text-only input) | Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview| import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, VertexAIBackend, ResponseModality } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported locationconst ai = getAI(firebaseApp, { backend: new VertexAIBackend('global') });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, {  model: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: {    responseModalities: [ResponseModality.TEXT, ResponseModality.IMAGE],  },});// Provide a text prompt instructing the model to generate an imageconst prompt = 'Generate an image of the Eiffel Tower with fireworks in the background.';// To generate an image, call \`generateContent\` with the text inputconst result = model.generateContent(prompt);// Handle the generated imagetry {  const inlineDataParts = result.response.inlineDataParts();  if (inlineDataParts?.[0]) {    const image = inlineDataParts[0].inlineData;    console.log(image.mimeType, image.data);  }} catch (err) {  console.error('Prompt or candidate was blocked:', err);}|
| Dart Flutter | Generate images (text-only input) | Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview| import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';await Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputfinal model = FirebaseAI.vertexAI(location: 'global').generativeModel(  model: 'gemini-2.5-flash-image-preview',  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [ResponseModalities.text, ResponseModalities.image]),);// Provide a text prompt instructing the model to generate an imagefinal prompt = [Content.text('Generate an image of the Eiffel Tower with fireworks in the background.')];// To generate an image, call \`generateContent\` with the text inputfinal response = await model.generateContent(prompt);if (response.inlineDataParts.isNotEmpty) {  final imageBytes = response.inlineDataParts[0].bytes;  // Process the image} else {  // Handle the case where no images were generated  print('Error: No images were generated.');}|
| Unity  | Generate images (text-only input) | Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview| using Firebase;using Firebase.AI;// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputvar model = FirebaseAI.GetInstance(FirebaseAI.Backend.VertexAI(location: "global")).GetGenerativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: new GenerationConfig(    responseModalities: new[] { ResponseModality.Text, ResponseModality.Image }));// Provide a text prompt instructing the model to generate an imagevar prompt = "Generate an image of the Eiffel Tower with fireworks in the background.";// To generate an image, call \`GenerateContentAsync\` with the text inputvar response = await model.GenerateContentAsync(prompt);var text = response.Text;if (!string.IsNullOrWhiteSpace(text)) {  // Do something with the text}// Handle the generated imagevar imageParts = response.Candidates.First().Content.Parts                         .OfType<ModelContent.InlineDataPart>()                         .Where(part => part.MimeType == "image/png");foreach (var imagePart in imageParts) {  // Load the Image into a Unity Texture2D object  UnityEngine.Texture2D texture2D = new(2, 2);  if (texture2D.LoadImage(imagePart.Data.ToArray())) {    // Do something with the image  }}|
| Swift iOS  | Iterate and edit images using multi-turn chat (nano banana)| Gemini Developer API (Developer API) gemini-2.5-flash-image-preview | import FirebaseAI// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputlet generativeModel = FirebaseAI.firebaseAI(backend: .googleAI()).generativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [.text, .image]))// Initialize the chatlet chat = model.startChat()guard let image = UIImage(named: "scones") else { fatalError("Image file not found.") }// Provide an initial text prompt instructing the model to edit the imagelet prompt = "Edit this image to make it look like a cartoon"// To generate an initial response, send a user message with the image and text promptlet response = try await chat.sendMessage(image, prompt)// Inspect the generated imageguard let inlineDataPart = response.inlineDataParts.first else {  fatalError("No image data in response.")}guard let uiImage = UIImage(data: inlineDataPart.data) else {  fatalError("Failed to convert data to UIImage.")}// Follow up requests do not need to specify the image againlet followUpResponse = try await chat.sendMessage("But make it old-school line drawing style")// Inspect the edited image after the follow up requestguard let followUpInlineDataPart = followUpResponse.inlineDataParts.first else {  fatalError("No image data in response.")}guard let followUpUIImage = UIImage(data: followUpInlineDataPart.data) else {  fatalError("Failed to convert data to UIImage.")}|
| Kotlin Android | Iterate and edit images using multi-turn chat (nano banana)| Gemini Developer API (Developer API) gemini-2.5-flash-image-preview |// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputval model = Firebase.ai(backend = GenerativeBackend.googleAI()).generativeModel(    modelName = "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    generationConfig = generationConfig {responseModalities = listOf(ResponseModality.TEXT, ResponseModality.IMAGE) })// Provide an image for the model to editval bitmap = BitmapFactory.decodeResource(context.resources, R.drawable.scones)// Create the initial prompt instructing the model to edit the imageval prompt = content {    image(bitmap)    text("Edit this image to make it look like a cartoon")}// Initialize the chatval chat = model.startChat()// To generate an initial response, send a user message with the image and text promptvar response = chat.sendMessage(prompt)// Inspect the returned imagevar generatedImageAsBitmap = response    .candidates.first().content.parts.filterIsInstance<ImagePart>().firstOrNull()?.image// Follow up requests do not need to specify the image againresponse = chat.sendMessage("But make it old-school line drawing style")generatedImageAsBitmap = response    .candidates.first().content.parts.filterIsInstance<ImagePart>().firstOrNull()?.image |
| Java Android | Iterate and edit images using multi-turn chat (nano banana)| Gemini Developer API (Developer API) gemini-2.5-flash-image-preview |// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.googleAI()).generativeModel(    "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    new GenerationConfig.Builder()        .setResponseModalities(Arrays.asList(ResponseModality.TEXT, ResponseModality.IMAGE))        .build());GenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide an image for the model to editBitmap bitmap = BitmapFactory.decodeResource(resources, R.drawable.scones);// Initialize the chatChatFutures chat = model.startChat();// Create the initial prompt instructing the model to edit the imageContent prompt = new Content.Builder()        .setRole("user")        .addImage(bitmap)        .addText("Edit this image to make it look like a cartoon")        .build();// To generate an initial response, send a user message with the image and text promptListenableFuture<GenerateContentResponse> response = chat.sendMessage(prompt);// Extract the image from the initial responseListenableFuture<@Nullable Bitmap> initialRequest = Futures.transform(response, result -> {    for (Part part : result.getCandidates().get(0).getContent().getParts()) {        if (part instanceof ImagePart) {            ImagePart imagePart = (ImagePart) part;            return imagePart.getImage();        }    }    return null;}, executor);// Follow up requests do not need to specify the image againListenableFuture<GenerateContentResponse> modelResponseFuture = Futures.transformAsync(        initialRequest,        generatedImage -> {            Content followUpPrompt = new Content.Builder()                    .addText("But make it old-school line drawing style")                    .build();            return chat.sendMessage(followUpPrompt);        },        executor);// Add a final callback to check the reworked imageFutures.addCallback(modelResponseFuture, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {        for (Part part : result.getCandidates().get(0).getContent().getParts()) {            if (part instanceof ImagePart) {                ImagePart imagePart = (ImagePart) part;                Bitmap generatedImageAsBitmap = imagePart.getImage();                break;            }        }    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor); |
| Web Modular API | Iterate and edit images using multi-turn chat (nano banana)| Gemini Developer API (Developer API) gemini-2.5-flash-image-preview |import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, GoogleAIBackend, ResponseModality } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Gemini Developer API backend serviceconst ai = getAI(firebaseApp, { backend: new GoogleAIBackend() });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, {  model: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: {    responseModalities: [ResponseModality.TEXT, ResponseModality.IMAGE],  },});// Prepare an image for the model to editasync function fileToGenerativePart(file) {  const base64EncodedDataPromise = new Promise((resolve) => {    const reader = new FileReader();    reader.onloadend = () => resolve(reader.result.split(',')[1]);    reader.readAsDataURL(file);  });  return {    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },  };}const fileInputEl = document.querySelector("input[type=file]");const imagePart = await fileToGenerativePart(fileInputEl.files[0]);// Provide an initial text prompt instructing the model to edit the imageconst prompt = "Edit this image to make it look like a cartoon";// Initialize the chatconst chat = model.startChat();// To generate an initial response, send a user message with the image and text promptconst result = await chat.sendMessage([prompt, imagePart]);// Request and inspect the generated imagetry {  const inlineDataParts = result.response.inlineDataParts();  if (inlineDataParts?.[0]) {    // Inspect the generated image    const image = inlineDataParts[0].inlineData;    console.log(image.mimeType, image.data);  }} catch (err) {  console.error('Prompt or candidate was blocked:', err);}// Follow up requests do not need to specify the image againconst followUpResult = await chat.sendMessage("But make it old-school line drawing style");// Request and inspect the returned imagetry {  const followUpInlineDataParts = followUpResult.response.inlineDataParts();  if (followUpInlineDataParts?.[0]) {    // Inspect the generated image    const followUpImage = followUpInlineDataParts[0].inlineData;    console.log(followUpImage.mimeType, followUpImage.data);  }} catch (err) {  console.error('Prompt or candidate was blocked:', err);} |
| Dart Flutter | Iterate and edit images using multi-turn chat (nano banana)| Gemini Developer API (Developer API) gemini-2.5-flash-image-preview | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';await Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputfinal model = FirebaseAI.googleAI().generativeModel(  model: 'gemini-2.5-flash-image-preview',  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [ResponseModalities.text, ResponseModalities.image]),);// Prepare an image for the model to editfinal image = await File('scones.jpg').readAsBytes();final imagePart = InlineDataPart('image/jpeg', image);// Provide an initial text prompt instructing the model to edit the imagefinal prompt = TextPart("Edit this image to make it look like a cartoon");// Initialize the chatfinal chat = model.startChat();// To generate an initial response, send a user message with the image and text promptfinal response = await chat.sendMessage([  Content.multi([prompt,imagePart])]);// Inspect the returned imageif (response.inlineDataParts.isNotEmpty) {  final imageBytes = response.inlineDataParts[0].bytes;  // Process the image} else {  // Handle the case where no images were generated  print('Error: No images were generated.');}// Follow up requests do not need to specify the image againfinal followUpResponse = await chat.sendMessage([  Content.text("But make it old-school line drawing style")]);// Inspect the returned imageif (followUpResponse.inlineDataParts.isNotEmpty) {  final followUpImageBytes = response.inlineDataParts[0].bytes;  // Process the image} else {  // Handle the case where no images were generated  print('Error: No images were generated.');}|
| Unity  | Iterate and edit images using multi-turn chat (nano banana)| Gemini Developer API (Developer API) gemini-2.5-flash-image-preview | using Firebase;using Firebase.AI;// Initialize the Gemini Developer API backend service// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputvar model = FirebaseAI.GetInstance(FirebaseAI.Backend.GoogleAI()).GetGenerativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: new GenerationConfig(    responseModalities: new[] { ResponseModality.Text, ResponseModality.Image }));// Prepare an image for the model to editvar imageFile = System.IO.File.ReadAllBytes(System.IO.Path.Combine(  UnityEngine.Application.streamingAssetsPath, "scones.jpg"));var image = ModelContent.InlineData("image/jpeg", imageFile);// Provide an initial text prompt instructing the model to edit the imagevar prompt = ModelContent.Text("Edit this image to make it look like a cartoon.");// Initialize the chatvar chat = model.StartChat();// To generate an initial response, send a user message with the image and text promptvar response = await chat.SendMessageAsync(new [] { prompt, image });// Inspect the returned imagevar imageParts = response.Candidates.First().Content.Parts                         .OfType<ModelContent.InlineDataPart>()                         .Where(part => part.MimeType == "image/png");// Load the image into a Unity Texture2D objectUnityEngine.Texture2D texture2D = new(2, 2);if (texture2D.LoadImage(imageParts.First().Data.ToArray())) {  // Do something with the image}// Follow up requests do not need to specify the image againvar followUpResponse = await chat.SendMessageAsync("But make it old-school line drawing style");// Inspect the returned imagevar followUpImageParts = followUpResponse.Candidates.First().Content.Parts                         .OfType<ModelContent.InlineDataPart>()                         .Where(part => part.MimeType == "image/png");// Load the image into a Unity Texture2D objectUnityEngine.Texture2D followUpTexture2D = new(2, 2);if (followUpTexture2D.LoadImage(followUpImageParts.First().Data.ToArray())) {  // Do something with the image}|
| Swift iOS  | Iterate and edit images using multi-turn chat (nano banana)| Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview | import FirebaseAI// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputlet generativeModel = FirebaseAI.firebaseAI(backend: .vertexAI(location: "global")).generativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [.text, .image]))// Initialize the chatlet chat = model.startChat()guard let image = UIImage(named: "scones") else { fatalError("Image file not found.") }// Provide an initial text prompt instructing the model to edit the imagelet prompt = "Edit this image to make it look like a cartoon"// To generate an initial response, send a user message with the image and text promptlet response = try await chat.sendMessage(image, prompt)// Inspect the generated imageguard let inlineDataPart = response.inlineDataParts.first else {  fatalError("No image data in response.")}guard let uiImage = UIImage(data: inlineDataPart.data) else {  fatalError("Failed to convert data to UIImage.")}// Follow up requests do not need to specify the image againlet followUpResponse = try await chat.sendMessage("But make it old-school line drawing style")// Inspect the edited image after the follow up requestguard let followUpInlineDataPart = followUpResponse.inlineDataParts.first else {  fatalError("No image data in response.")}guard let followUpUIImage = UIImage(data: followUpInlineDataPart.data) else {  fatalError("Failed to convert data to UIImage.")}|
| Kotlin Android | Iterate and edit images using multi-turn chat (nano banana)| Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview | // Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputval model = Firebase.ai(backend = GenerativeBackend.vertexAI(location = "global")).generativeModel(    modelName = "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    generationConfig = generationConfig {responseModalities = listOf(ResponseModality.TEXT, ResponseModality.IMAGE) })// Provide an image for the model to editval bitmap = BitmapFactory.decodeResource(context.resources, R.drawable.scones)// Create the initial prompt instructing the model to edit the imageval prompt = content {    image(bitmap)    text("Edit this image to make it look like a cartoon")}// Initialize the chatval chat = model.startChat()// To generate an initial response, send a user message with the image and text promptvar response = chat.sendMessage(prompt)// Inspect the returned imagevar generatedImageAsBitmap = response    .candidates.first().content.parts.filterIsInstance<ImagePart>().firstOrNull()?.image// Follow up requests do not need to specify the image againresponse = chat.sendMessage("But make it old-school line drawing style")generatedImageAsBitmap = response    .candidates.first().content.parts.filterIsInstance<ImagePart>().firstOrNull()?.image|
| Java Android | Iterate and edit images using multi-turn chat (nano banana)| Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview | // Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.vertexAI("global")).generativeModel(    "gemini-2.5-flash-image-preview",    // Configure the model to respond with text and images (required)    new GenerationConfig.Builder()        .setResponseModalities(Arrays.asList(ResponseModality.TEXT, ResponseModality.IMAGE))        .build());GenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide an image for the model to editBitmap bitmap = BitmapFactory.decodeResource(resources, R.drawable.scones);// Initialize the chatChatFutures chat = model.startChat();// Create the initial prompt instructing the model to edit the imageContent prompt = new Content.Builder()        .setRole("user")        .addImage(bitmap)        .addText("Edit this image to make it look like a cartoon")        .build();// To generate an initial response, send a user message with the image and text promptListenableFuture<GenerateContentResponse> response = chat.sendMessage(prompt);// Extract the image from the initial responseListenableFuture<@Nullable Bitmap> initialRequest = Futures.transform(response, result -> {    for (Part part : result.getCandidates().get(0).getContent().getParts()) {        if (part instanceof ImagePart) {            ImagePart imagePart = (ImagePart) part;            return imagePart.getImage();        }    }    return null;}, executor);// Follow up requests do not need to specify the image againListenableFuture<GenerateContentResponse> modelResponseFuture = Futures.transformAsync(        initialRequest,        generatedImage -> {            Content followUpPrompt = new Content.Builder()                    .addText("But make it old-school line drawing style")                    .build();            return chat.sendMessage(followUpPrompt);        },        executor);// Add a final callback to check the reworked imageFutures.addCallback(modelResponseFuture, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {        for (Part part : result.getCandidates().get(0).getContent().getParts()) {            if (part instanceof ImagePart) {                ImagePart imagePart = (ImagePart) part;                Bitmap generatedImageAsBitmap = imagePart.getImage();                break;            }        }    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor);|
| Web Modular API | Iterate and edit images using multi-turn chat (nano banana)| Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, VertexAIBackend, ResponseModality } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported locationconst ai = getAI(firebaseApp, { backend: new VertexAIBackend('global') });// Create a \`GenerativeModel\` instance with a model that supports your use caseconst model = getGenerativeModel(ai, {  model: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: {    responseModalities: [ResponseModality.TEXT, ResponseModality.IMAGE],  },});// Prepare an image for the model to editasync function fileToGenerativePart(file) {  const base64EncodedDataPromise = new Promise((resolve) => {    const reader = new FileReader();    reader.onloadend = () => resolve(reader.result.split(',')[1]);    reader.readAsDataURL(file);  });  return {    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },  };}const fileInputEl = document.querySelector("input[type=file]");const imagePart = await fileToGenerativePart(fileInputEl.files[0]);// Provide an initial text prompt instructing the model to edit the imageconst prompt = "Edit this image to make it look like a cartoon";// Initialize the chatconst chat = model.startChat();// To generate an initial response, send a user message with the image and text promptconst result = await chat.sendMessage([prompt, imagePart]);// Request and inspect the generated imagetry {  const inlineDataParts = result.response.inlineDataParts();  if (inlineDataParts?.[0]) {    // Inspect the generated image    const image = inlineDataParts[0].inlineData;    console.log(image.mimeType, image.data);  }} catch (err) {  console.error('Prompt or candidate was blocked:', err);}// Follow up requests do not need to specify the image againconst followUpResult = await chat.sendMessage("But make it old-school line drawing style");// Request and inspect the returned imagetry {  const followUpInlineDataParts = followUpResult.response.inlineDataParts();  if (followUpInlineDataParts?.[0]) {    // Inspect the generated image    const followUpImage = followUpInlineDataParts[0].inlineData;    console.log(followUpImage.mimeType, followUpImage.data);  }} catch (err) {  console.error('Prompt or candidate was blocked:', err);}|
| Dart Flutter | Iterate and edit images using multi-turn chat (nano banana)| Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';await Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputfinal model = FirebaseAI.vertexAI(location: 'global').generativeModel(  model: 'gemini-2.5-flash-image-preview',  // Configure the model to respond with text and images (required)  generationConfig: GenerationConfig(responseModalities: [ResponseModalities.text, ResponseModalities.image]),);// Prepare an image for the model to editfinal image = await File('scones.jpg').readAsBytes();final imagePart = InlineDataPart('image/jpeg', image);// Provide an initial text prompt instructing the model to edit the imagefinal prompt = TextPart("Edit this image to make it look like a cartoon");// Initialize the chatfinal chat = model.startChat();// To generate an initial response, send a user message with the image and text promptfinal response = await chat.sendMessage([  Content.multi([prompt,imagePart])]);// Inspect the returned imageif (response.inlineDataParts.isNotEmpty) {  final imageBytes = response.inlineDataParts[0].bytes;  // Process the image} else {  // Handle the case where no images were generated  print('Error: No images were generated.');}// Follow up requests do not need to specify the image againfinal followUpResponse = await chat.sendMessage([  Content.text("But make it old-school line drawing style")]);// Inspect the returned imageif (followUpResponse.inlineDataParts.isNotEmpty) {  final followUpImageBytes = response.inlineDataParts[0].bytes;  // Process the image} else {  // Handle the case where no images were generated  print('Error: No images were generated.');}|
| Unity  | Iterate and edit images using multi-turn chat (nano banana)| Vertex AI Gemini API (Vertex AI) gemini-2.5-flash-image-preview | using Firebase;using Firebase.AI;// Initialize the Vertex AI Gemini API backend service// Specify the location to access the model — for preview models, \`global\` is usually the only supported location// Create a \`GenerativeModel\` instance with a Gemini model that supports image outputvar model = FirebaseAI.GetInstance(FirebaseAI.Backend.VertexAI(location: "global")).GetGenerativeModel(  modelName: "gemini-2.5-flash-image-preview",  // Configure the model to respond with text and images (required)  generationConfig: new GenerationConfig(    responseModalities: new[] { ResponseModality.Text, ResponseModality.Image }));// Prepare an image for the model to editvar imageFile = System.IO.File.ReadAllBytes(System.IO.Path.Combine(  UnityEngine.Application.streamingAssetsPath, "scones.jpg"));var image = ModelContent.InlineData("image/jpeg", imageFile);// Provide an initial text prompt instructing the model to edit the imagevar prompt = ModelContent.Text("Edit this image to make it look like a cartoon.");// Initialize the chatvar chat = model.StartChat();// To generate an initial response, send a user message with the image and text promptvar response = await chat.SendMessageAsync(new [] { prompt, image });// Inspect the returned imagevar imageParts = response.Candidates.First().Content.Parts                         .OfType<ModelContent.InlineDataPart>()                         .Where(part => part.MimeType == "image/png");// Load the image into a Unity Texture2D objectUnityEngine.Texture2D texture2D = new(2, 2);if (texture2D.LoadImage(imageParts.First().Data.ToArray())) {  // Do something with the image}// Follow up requests do not need to specify the image againvar followUpResponse = await chat.SendMessageAsync("But make it old-school line drawing style");// Inspect the returned imagevar followUpImageParts = followUpResponse.Candidates.First().Content.Parts                         .OfType<ModelContent.InlineDataPart>()                         .Where(part => part.MimeType == "image/png");// Load the image into a Unity Texture2D objectUnityEngine.Texture2D followUpTexture2D = new(2, 2);if (followUpTexture2D.LoadImage(followUpImageParts.First().Data.ToArray())) {  // Do something with the image}|
          `,
        },
      ],
    };
  },
);
